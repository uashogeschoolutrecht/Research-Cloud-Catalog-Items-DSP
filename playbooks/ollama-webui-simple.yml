---
# Ollama with Open-WebUI installation for SURF Research Cloud
# Simplified version using docker commands instead of community.docker collection
# Run with: ansible-playbook ollama-webui-simple.yml

- name: Install Ollama with Open-WebUI
  hosts: localhost
  become: yes
  connection: local
  gather_facts: yes

  vars:
    # Service ports
    ollama_port: 11434
    webui_port: 3000
    n8n_port: 5678
    
    # Ollama configuration
    ollama_host: "0.0.0.0"
    ollama_models:
      - "gemma2:2b"      # Small, fast model (~1.6GB)
      - "llama3.1:8b"    # Balanced model (~4.7GB)
    
    # Open-WebUI configuration
    webui_auth: false  # Disabled for ease of use on shared workspaces
    webui_image: "ghcr.io/open-webui/open-webui:cuda"
    webui_volume: "open-webui-data"
    
    # N8N configuration (optional)
    install_n8n: false  # Set to true to install N8N
    n8n_image: "docker.n8n.io/n8nio/n8n"
    n8n_volume: "n8n_data"
    
    # Directories
    compose_dir: "/opt/ollama-stack"

  tasks:
    # === DOCKER INSTALLATION ===
    - name: Install Docker prerequisites
      apt:
        name:
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
        state: present

    - name: Create Docker keyring directory
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: Add Docker GPG key
      shell: |
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
        chmod a+r /etc/apt/keyrings/docker.asc
      args:
        creates: /etc/apt/keyrings/docker.asc

    - name: Add Docker repository
      shell: |
        echo \
          "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
          $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
          tee /etc/apt/sources.list.d/docker.list > /dev/null
      args:
        creates: /etc/apt/sources.list.d/docker.list

    - name: Update apt cache after adding Docker repository
      apt:
        update_cache: yes

    - name: Install Docker Engine
      apt:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-buildx-plugin
          - docker-compose-plugin
        state: present

    - name: Start and enable Docker service
      systemd:
        name: docker
        state: started
        enabled: yes

    - name: Add user to docker group
      user:
        name: "{{ ansible_user }}"
        groups: docker
        append: yes

    # === NVIDIA CONTAINER TOOLKIT ===
    - name: Add NVIDIA Container Toolkit GPG key
      shell: |
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
        gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      args:
        creates: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

    - name: Add NVIDIA Container Toolkit repository
      shell: |
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
        tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
      args:
        creates: /etc/apt/sources.list.d/nvidia-container-toolkit.list

    - name: Update apt cache after adding NVIDIA repository
      apt:
        update_cache: yes

    - name: Install NVIDIA Container Toolkit
      apt:
        name: nvidia-container-toolkit
        state: present

    - name: Configure NVIDIA Container Runtime
      shell: nvidia-ctk runtime configure --runtime=docker
      register: nvidia_config
      changed_when: nvidia_config.rc == 0

    - name: Restart Docker after NVIDIA configuration
      systemd:
        name: docker
        state: restarted
      when: nvidia_config.changed

    # === OLLAMA INSTALLATION ===
    - name: Install Ollama
      shell: curl -fsSL https://ollama.com/install.sh | sh
      args:
        creates: /usr/local/bin/ollama

    - name: Create Ollama systemd override directory
      file:
        path: /etc/systemd/system/ollama.service.d
        state: directory
        mode: '0755'

    - name: Configure Ollama environment for Docker communication
      copy:
        content: |
          [Service]
          Environment="OLLAMA_HOST={{ ollama_host }}:{{ ollama_port }}"
        dest: /etc/systemd/system/ollama.service.d/override.conf
        mode: '0644'
      register: ollama_config

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes
      when: ollama_config.changed

    - name: Start and enable Ollama service
      systemd:
        name: ollama
        state: started
        enabled: yes

    - name: Restart Ollama service if config changed
      systemd:
        name: ollama
        state: restarted
      when: ollama_config.changed

    - name: Wait for Ollama to be ready
      wait_for:
        port: "{{ ollama_port }}"
        host: "{{ ollama_host }}"
        delay: 5
        timeout: 60

    # === DOCKER COMPOSE SETUP ===
    - name: Create compose directory
      file:
        path: "{{ compose_dir }}"
        state: directory
        mode: '0755'

    - name: Create Docker Compose file
      copy:
        content: |
          version: '3.8'

          services:
            open-webui:
              image: {{ webui_image }}
              container_name: open-webui
              ports:
                - "{{ webui_port }}:8080"
              volumes:
                - {{ webui_volume }}:/app/backend/data
              environment:
                - OLLAMA_BASE_URL=http://host.docker.internal:{{ ollama_port }}
          {% if not webui_auth %}
                - WEBUI_AUTH=False
          {% endif %}
              extra_hosts:
                - "host.docker.internal:host-gateway"
              deploy:
                resources:
                  reservations:
                    devices:
                      - driver: nvidia
                        count: all
                        capabilities: [gpu]
              restart: unless-stopped
              networks:
                - ollama-network

          {% if install_n8n %}
            n8n:
              image: {{ n8n_image }}
              container_name: n8n
              ports:
                - "{{ n8n_port }}:5678"
              volumes:
                - {{ n8n_volume }}:/home/node/.n8n
              extra_hosts:
                - "host.docker.internal:host-gateway"
              restart: unless-stopped
              networks:
                - ollama-network
          {% endif %}

          volumes:
            {{ webui_volume }}:
              driver: local
          {% if install_n8n %}
            {{ n8n_volume }}:
              driver: local
          {% endif %}

          networks:
            ollama-network:
              driver: bridge
        dest: "{{ compose_dir }}/docker-compose.yml"
        mode: '0644'

    - name: Start Open-WebUI with Docker Compose (using docker compose command)
      shell: |
        cd {{ compose_dir }}
        docker compose up -d
      become_user: "{{ ansible_user }}"

    - name: Wait for Open-WebUI to be ready
      wait_for:
        port: "{{ webui_port }}"
        host: "0.0.0.0"
        delay: 10
        timeout: 120

    # === INSTALL OLLAMA MODELS ===
    - name: Install default models
      shell: "ollama pull {{ item }}"
      loop: "{{ ollama_models }}"
      become_user: "{{ ansible_user }}"
      register: model_install
      changed_when: "'already exists' not in model_install.stderr"
      failed_when: model_install.rc != 0 and 'already exists' not in model_install.stderr

  post_tasks:
    - name: Display service information
      debug:
        msg:
          - "üéâ Installation completed successfully!"
          - ""
          - "üìç Access your services:"
          - "  ‚Ä¢ Open-WebUI: http://{{ ansible_default_ipv4.address }}:{{ webui_port }}"
          - "  ‚Ä¢ Ollama API: http://{{ ansible_default_ipv4.address }}:{{ ollama_port }}"
          - "{% if install_n8n %}  ‚Ä¢ N8N: http://{{ ansible_default_ipv4.address }}:{{ n8n_port }}{% endif %}"
          - ""
          - "ü§ñ Default models installed: {{ ollama_models | join(', ') }}"
          - "üìö Install more models: ollama pull <model-name>"
          - "üí° Available models: https://ollama.com/library"
